<!DOCTYPE html>
<html>

<head>
    <meta charset="utf-8">
    <meta name="description"
        content="WalkVLM-LR">
    <meta name="keywords" content="Text-to-Video, Diffusion Model">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>WalkVLM-LR</title>

    <script type="text/x-mathjax-config">
    MathJax.Hub.Config({tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}});
    </script>
    <script type="text/javascript"
        src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
        </script>

    <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">

    <link rel="stylesheet" href="./static/css/bulma.min.css">
    <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
    <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
    <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
    <link rel="stylesheet" href="./static/css/index.css">
    <link rel="icon" href="./static/images/favicon.svg">

    <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
    <script defer src="./static/js/fontawesome.all.min.js"></script>
    <script src="./static/js/bulma-carousel.min.js"></script>
    <script src="./static/js/bulma-slider.min.js"></script>
    <script src="./static/js/index.js"></script>
</head>

<body>


<section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h2 class="title is-2 publication-title">
                <!-- <img class="center" width="100" height="35" src=""> -->
                Less Redundancy: Boosting Practicality of Vision Language Model in Walking Assistants
            </h2>
            <div class="is-size-5 publication-authors">

                            <div class="column has-text-centered">
              <div class="publication-links">


                <span class="link-block">
                  <a href=""
                     class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                        <i class="ai ai-arxiv"></i>
                    </span>
                    <span>arXiv</span>
                  </a>
                </span>

                <span class="link-block">
                  <a href="https://sprproxy-1258344707.cos.ap-shanghai.myqcloud.com/seraphyuan/ilabel/vsd2m_full.zip"
                     class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                        <i class="fab fa-github"></i>
                    </span>
                    <span>Data (1.3T)</span>
                    </a>
                </span>                  

                <span class="link-block">
                  <a href="https://github.com/WalkVLM-LR/WalkVLM-LR.github.io"
                     class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                        <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                    </a>
                </span>
              </div>
 <!--
              <span class="author-block">
                <a href="https://chenhsing.github.io">Zhen Xing</a><sup>1</sup>,</span>
              <span class="author-block">
                Qi Dai<sup>2</sup>,</span>
              <span class="author-block">
                Han Hu<sup>2</sup>,
              </span>
              <span class="author-block">
                Zuxuan Wu<sup>1</sup>,
              </span>
              <span class="author-block">
                Yu-Gang Jiang<sup>1</sup>
              </span>

            <div class="is-size-5 publication-authors">
              <span class="author-block"><sup>1</sup> Fudan University,</span>
              <span class="author-block"><sup>2</sup>MicroSoft Research Asia</span>
            </div>

            <div class="column has-text-centered">
              <div class="publication-links">

                <span class="link-block">
                  <a href="https://arxiv.org/abs/2308.09710"
                     class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                        <i class="fas fa-file-pdf"></i>
                    </span>
                    <span>Paper</span>
                  </a>
                </span>
                <span class="link-block">
                  <a href="https://arxiv.org/abs/2308.09710"
                     class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                        <i class="ai ai-arxiv"></i>
                    </span>
                    <span>arXiv</span>
                  </a>
                </span>

                <span class="link-block">
                  <a href="https://github.com/ChenHsing/SimDA"
                     class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                        <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                    </a>
                </span>
-->
              </div>
  
            </div>
          </div>
        </div>
      </div>
    </div>
  </section>
  


  <section class="section">
    <div class="container is-max-desktop">
      <!-- Abstract. -->
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Abstract</h2>
          <div class="content has-text-justified">
            <p>
                Approximately 283 million people worldwide live with visual impairments, motivating increasing research into leveraging Visual Language Models (VLMs) to develop effective walking assistance systems for blind and low vision individuals.
                However, existing VLMs in walking assistant task often have outputs that contain considerable redundancy and extraneous details, adversely affecting users' ability to accurately assess their surroundings.
                Moreover, these models typically lack the capability to proactively assess environmental risks and  adaptively trigger reminders based on the appropriate scene, leading to excessive temporal redundancy.
            </p>
            <p>
                To mitigate output and temporal redundancy, we propose WalkVLM-LR, a walking assistance model with less redundancy. 
To reduce output redundancy, we introduce four human-preference-based custom reward functions within the GRPO-based reasoning framework to optimize the output in terms of conciseness, fluency, keyword density, and accuracy, thereby producing more informative and streamlined outputs.
To minimize temporal redundancy, we incorporate an environment awareness discriminator, which shares the visual encoder with the VLMs to reduce redundant computations and enhance discriminative efficiency, to make WalkVLM-LR assess scene risk levels and minimize unnecessary reminders.
Experimental results demonstrate that our method achieves state-of-the-art performance across all evaluation metrics compared with other models, particularly in output conciseness and less temporal redundancy.
            </p>
          </div>
        </div>
      </div>
      <!--/ Abstract. -->


    <section class="section" id="methods">
        <hr />
        <h2 class="title is-3">Method</h2>

        <hr />
            <td align="center" style="padding-left: 0px; padding-bottom: 0px;"><img class="center" width="1280"
                    height="160" src="assets/framework.jpg"></td>
        <center>
            <h5 class="title is-6"><strong>Figure 1:</strong>The architecture of WalkVLM-LR. The top-right of the image illustrates how WalkVLM-LR operates, while the left side depicts its overall design. The bottom-right shows the structure of the EAD module.</h5>
        </center>

        <hr />
            <td align="center" style="padding-left: 0px; padding-bottom: 0px;"><img class="center" width="1280"
                    height="160" src="assets/GRPO.jpg"></td>
        <center>
            <h5 class="title is-6"><strong>Figure 2:</strong>Four Rewards in GRPO Training. GRPO utilizes an intra-group relative advantage evaluation mechanism, combined with four customized reward functions, to assist the VLM in learning how to generate outputs that align with human preferences.</h5>
        </center>
    </section>
    <section class="section" id="results" style="margin-top: 10px;">
  <hr />
  <h2 class="title is-3">Results</h2>

  <div style="text-align: center; margin-bottom: 2rem;">
    <a href="assets/number.png" target="_blank">
      <img src="assets/number.png" style="max-height: 500px; width: auto; cursor: zoom-in;" alt="Results Table">
    </a>
    <h5 class="title is-6"><strong>Table 1:</strong>Quantitative comparison of different methods. WalkVLM-LR achieves the best performance across ROUGE, Keyword Density, and GPT Score metrics. These results highlight the effectiveness of WalkVLM-LR in reducing redundancy while maintaining high-quality, concise outputs, making it a superior choice for walking assistant applications.</h5>
  </div>

  <div style="text-align: center; margin-bottom: 2rem;">
    <a href="assets/zhuzhuang.jpg" target="_blank">
      <img src="assets/zhuzhuang.jpg" style="max-height: 550px; width: auto; cursor: zoom-in;" alt="WeChat Case 1">
    </a>
    <h5 class="title is-6"><strong>Figure 3:</strong>The comparison between WalkVLM-LR and mainstream VLMs in terms of output length and keyword density. The outputs of WalkVLM-LR are the shortest and exhibit the highest keyword density, containing the least amount of redundant information.</h5>
  </div>

  <div style="text-align: center; margin-bottom: 2rem;">
    <a href="assets/visual.jpg" target="_blank">
      <img src="assets/visual.jpg" style="max-height: 550px; width: auto; cursor: zoom-in;" alt="MLDR Qualitative Results">
    </a>
    <h5 class="title is-6"><strong>Figure 4:</strong>Output comparison of different VLM models in walking assistant task. Compared to other models, WalkVLM-LR generates concise and informative responses, offering better guidance for visually impaired users.</h5>
  </div>

  <div style="text-align: center; margin-bottom: 2rem;">
    <a href="assets/vis2.jpg" target="_blank">
      <img src="assets/vis2.jpg" style="max-height: 550px; width: auto; cursor: zoom-in;" alt="MLDR Qualitative Results">
    </a>
    <h5 class="title is-6"><strong>Figure 5:</strong>Output comparison of different VLM models in walking assistant task. Compared to other models, WalkVLM-LR generates concise and informative responses, offering better guidance for visually impaired users.</h5>
  </div>
    
    </section>


    <section class="section" id="further work">
        <hr />
        <h2 class="title is-3">Further Work</h2>

        <hr />
            <td align="center" style="padding-left: 0px; padding-bottom: 0px;"><img class="center" width="1280"
                    height="160" src="assets/bad.jpg"></td>
        <center>
            <h5 class="title is-6"><strong>Figure 6:</strong>Examples of WalkVLM Underperformance. WalkVLM-LR analysis reveals issues such as incomplete target detection, incorrect directional information, and poor focus on nearby obstacles. It tends to emphasize distant objects while neglecting immediate hazards, especially in low-light environments, and generates verbose, uninformative reminders in complex scenes. Future improvements should enhance obstacle detection, refine spatial localization, and integrate an attention mechanism to prioritize critical threats.</h5>



        </center>

    </section>


    <section class="section" id="BibTeX">
        <div class="container is-max-desktop content">
          <h2 class="title">BibTeX</h2>
          <p> If you use our work in your research, please cite: </p>
          <pre><code>
            @misc{anonymous2025Less Redundancy,
            title={Less Redundancy: Boosting Practicality of Vision Language Model in Walking Assistants},
            author={Anonymous},
            archivePrefix={arXiv},
            primaryClass={cs.CV}}
      </code></pre>
        </div>
      </section>



<!--
    <footer class="footer">
        <div class="container">
            <div class="content has-text-centered">
                <a class="icon-link" href="#">
                    <i class="fas fa-file-pdf"></i>
                </a>
            </div>
            <div class="columns is-centered">
                <div class="column is-8">
                    <div class="content">
                        <p>[1] Jonathan Ho, Tim Salimans, Alexey A Gritsenko, William Chan, Mohammad Norouzi, and David
                            J Fleet. Video
                            diffusion models. In
                            ICLR, 2022.</p>
                        <p>[2] Wenyi Hong, Ming Ding, Wendi Zheng, Xinghan Liu, and Jie Tang. Cogvideo: Large-scale
                            pretraining for
                            text-to-video generation
                            via transformers. In ICLR, 2023.</p>
                        <p>[3] Luo, Zhengxiong and Chen, Dayou and Zhang, Yingya and Huang, Yan and Wang, Liang and Shen, Yujun and Zhao, Deli and Zhou, Jingren and Tan, Tieniu
                            VideoFusion: Decomposed Diffusion Models for High-Quality Video Generation. In CVPR, 2023.</p>
                            
                        <p>[4] He, Yingqing, et al. "Latent video diffusion models for high-fidelity video generation with arbitrary lengths." arXiv preprint(2022).</p>
                            
        
                        <hr>
                        <p>
                            Tempolate from <a href="https://github.com/nerfies/nerfies.github.io">Nerfies</a>, thanks!
                        </p>
                    </div>
                </div>
            </div>
        </div>
    </footer>
-->
    <footer class="footer">
        <div class="container">
            <div class="content has-text-centered">
                <a class="icon-link" href="#">
                    <i class="fas fa-file-pdf"></i>
                </a>
            </div>
            <div class="columns is-centered">
                <div class="column is-8">
                    <div class="content">
                        <p>[1] Yuan, Z., Zhang, T., Deng, Y., Zhang, J., Zhu, Y., Jia, Z., Zhou, J., & Zhang, J. (2024). WalkVLM: Aid visually impaired people walking by vision language model. arXiv preprint arXiv:2412.2090.</p>

                        <p>[2] Hao, Y., Yang, F., Huang, H., Yuan, S., Rangan, S., Rizzo, J., Wang, Y., & Fang, Y. (2024). A multi-modal foundation model to assist people with blindness and low vision in environmental interaction. Journal of Imaging, 10(5), 103.</p>

                        <p>[3] Shao, Z., Wang, P., Zhu, Q., Xu, R., Song, J., Bi, X., Zhang, H., Zhang, M., Li, Y., Wu, Y., et al. (2024). DeepSeekMath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300.</p>
                        <hr>
                    </div>
                </div>
            </div>
        </div>
    </footer>


</body>
